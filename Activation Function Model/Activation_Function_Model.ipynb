{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  FIT3181: Deep Learning (S2, 2020)</span>\n",
    "***\n",
    "*CE & Lecturer:* Prof **Dinh Phung** | dinh.phung@monash.edu <br/>\n",
    "*Head of Tutor:* Dr **Ethan Zhao** | ethan.zhao@monash.edu <br/>\n",
    "<br/>\n",
    "Department of Data Science and AI, Faculty of Information Technology, Monash University, Australia\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  Student Information</span>\n",
    "***\n",
    "Surname: **[Patel]**  <br/>\n",
    "Firstname: **[Utkarsh]**    <br/>\n",
    "Student ID: **[29143926]**    <br/>\n",
    "Email: **[upat0003@student.monash.edu]**    <br/>\n",
    "Your tutorial time: **[Tue: 9-11 am]**    <br/>\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Theory and Knowledge Questions</span>\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.1.**</span> **Activation function plays an important role in modern Deep NNs. For each of the activation function below, state its output range and find its derivative (show your steps)**\n",
    "\n",
    "<span style=\"color:red\">**(a)**</span> Sigmoid: $\\sigma(x) = \\frac{1}{1+\\text{exp}{(-x)}}$ \n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div> \n",
    "\n",
    "<span style=\"color:red\">**(b)**</span> Tanh: $\\sigma(x) = \\frac{\\exp(x) - \\exp{(-x)}}{\\exp(x) + \\exp{(-x)}}= \\frac{1-\\exp(-2x)}{1+\\exp(2x)}$\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[1.5 points]</span></div> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "a)\n",
    "sig(x) = 1/1+exp(-x)\n",
    "\n",
    "The output range of sigmoid function, Range(sig(x)) = (0,1)\n",
    "\n",
    "\n",
    "\n",
    "For derivative, we have to find:\n",
    "\n",
    "d/dx [1 / (1+exp(-x))]\n",
    "\n",
    "= (-1) * (d/dx [exp(-x) +1] / (exp(-x) +1) ^2\n",
    "\n",
    "= (-1) * (d/dx [exp(-x)] + d/dx [1]) / (exp(-x) +1) ^2\n",
    "\n",
    "= (‚àí1) * (exp(-x) * d/dx [‚àíx] +0) / (exp(-x) +1) ^2\n",
    "\n",
    "= ‚àí(‚àíd/dx [x]) * exp(-x) / (exp(-x) + 1) ^2\n",
    "\n",
    "= 1 * exp(-x) / (exp(-x) + 1) ^2\n",
    "\n",
    "= exp(-x) / (exp(-x) + 1) ^2\n",
    "\n",
    "= exp(x) / (exp(x) + 1) ^2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "b)\n",
    "Tanh(x) = (1 ‚Äì exp(-2x)) / (1 + exp(2x))\n",
    "\n",
    "The output range of Tanh function, Range(Tanh(x)): If y = Tanh(x)\n",
    "\n",
    "Range = {y E R : y <= sqrt(2) / ((1 + sqrt(2)) * (2 + sqrt(2))) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For derivative, we have to find:\n",
    "\n",
    "d/dx [(1 ‚àí exp(‚àí2x)) / (1 + exp(2x))]\n",
    "\n",
    "= (d/dx [1 ‚àí exp(‚àí2x)] * (exp(2x) + 1) ‚àí (1 ‚àí exp(‚àí2x)) * d/dx [exp(2x) + 1]) / (exp(2x) + 1) ^ 2\n",
    "\n",
    "= ((d/dx [1] ‚àí d/dx [exp(‚àí2x)]) * (exp(2x) + 1) ‚àí (1 ‚àí exp(‚àí2x)) * (d/dx [exp(2x)] + d/dx [1])) / (exp(2x) + 1) ^ 2\n",
    "\n",
    "= (((0 ‚àí exp(‚àí2x) * d/dx[‚àí2x]) * (exp(2x)+1)) ‚Äì ((1‚àíexp(‚àí2x)) * (exp(2x) * d/dx[2x]+0))) / (exp(2x)+1) ^ 2\n",
    "\n",
    "= ‚àí(((‚àí2 * d/dx[x]) * exp(‚àí2x) * (exp(2x)+1)) ‚Äì ((1‚àíexp(‚àí2x)) * 2 * d/dx[x] * exp(2x))) / (exp(2x)+1) ^ 2\n",
    "\n",
    "= (2 * 1 * exp(‚àí2x) * (exp(2x) + 1) ‚àí 2(1‚àíexp(‚àí2x)) * 1exp(2x)) / (exp(2x) + 1) ^ 2\n",
    "\n",
    "=(2exp(‚àí2x) * (exp(2x) +1) ‚àí 2(1‚àíexp(‚àí2x)) * exp(2x)) / (exp(2x) + 1) ^ 2\n",
    "\n",
    "=(2exp(‚àí2x) / (exp(2x) + 1)) ‚Äì (2(1‚àíexp(‚àí2x)) * exp(2x) / (exp(2x) + 1) ^ 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:#0b486b\"> **Numpy is going to be used in the following questions. You need to import numpy here.** </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.2.**</span> **Softmax activation aims to transform discriminative values to prediction probabilities. Assume that at the output layer, we obtain the logit $h^{L}$ as shown in the following cell. What is the corresonding prediction probabilities $p$? You are required to answer the formula for $p$ and write the code for computing $p$ in the following cells.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[4 points]</span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Provide the formula for $p$ here**</span>\n",
    "\n",
    "- p = [p_m] = exp(h_m) / sum(exp(h_i)), where i=1 to M and and 1 <= m <=M, M=3 here \n",
    "\n",
    "sum(p_m)=1 and p_m >= 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the code to generate the logit $h^L$ using your *student ID* as the seed of numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24411655]\n",
      " [0.40021216]\n",
      " [0.72809061]]\n",
      "[[0.61632919]\n",
      " [0.72045058]\n",
      " [1.        ]]\n",
      "[[0.26375151]\n",
      " [0.30830915]\n",
      " [0.42793934]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29143926)  #replace 123456 with your student ID\n",
    "hL = np.random.rand(3,1) \n",
    "#Insert your code to compute the prediction probability here\n",
    "\n",
    "\"\"\"Compute softmax values for each sets of scores in x.\"\"\" \n",
    "\n",
    "print(hL)\n",
    "e_hL = np.exp(hL - np.max(hL)) \n",
    "print(e_hL)\n",
    "\n",
    "p = e_hL / e_hL.sum(axis=0) \n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.3.**</span> **Linear operation and element-wise activation are two building-blocks for conducting a layer in a feedforward neural network.**\n",
    "\n",
    "<span style=\"color:red\">**(a)**</span> **Assume that hidden layer $1$ has value $h^1(x) \\in \\mathbb{R}^{2 \\times 1}$ whose value will be generated randomly and the weight matrix and bias at the second layer are:**\n",
    "- **$W^{2} \\in \\mathbb{R}^{3 \\times 2}$ and $b^2 \\in \\mathbb{R}^{3 \\times 1}$ whose values are generated randomly too.\n",
    "What is the value of the hidden layer $\\bar{h}^{2}(x)$ after applying *the linear operation* with the matrix $W^2$ and the bias $b^2$ over $h^1$. Provide the formula and the code in the following cells.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> \n",
    "\n",
    "\n",
    "<span style=\"color:red\">**(b)**</span>**Assume that we apply *the ReLU activation function* at the second layer. What is the value of the hidden layer $h^2(x)$ after we apply the activation function? Provide the formula and the code in the following cells.** \n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(a) Provide the formula for $\\bar{h}^{2}(x)$ here**</span>\n",
    "\n",
    "- h_2(x) = w^2 * h^1(x) + ùëè^2 ‚àà ‚Ñù3√ó1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24411655 0.40021216]\n",
      " [0.72809061 0.60515694]\n",
      " [0.90809673 0.27014988]]\n",
      "[[0.37514391]\n",
      " [0.31110578]]\n",
      "[[0.01586411]\n",
      " [0.08587969]\n",
      " [0.14311617]]\n",
      "[[0.23195127]\n",
      " [0.54728627]\n",
      " [0.56782832]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(29143926)  #replace 123456 with your student ID\n",
    "W2 = np.random.rand(3,2)\n",
    "b2 = np.random.rand(3,1)\n",
    "#Insert your code to compute h1 and h{bar}2 here\n",
    "\n",
    "h1 = np.random.rand(2,1) \n",
    "\n",
    "h_2 = W2.dot(h1) + b2 \n",
    "print(W2)\n",
    "print(h1)\n",
    "print(b2)\n",
    "\n",
    "print(h_2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(b) Provide the formula for ${h}^{2}(x)$ here**</span>\n",
    "\n",
    "- h^2(x) = ReLU (h_2(x)) ‚àà ‚Ñù3√ó1\n",
    "OR h^2(x) = maximum(0,h_2(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23195127]\n",
      " [0.54728627]\n",
      " [0.56782832]]\n"
     ]
    }
   ],
   "source": [
    "#Insert your code to compute h2 here\n",
    "#Note that we use ReLU activation function\n",
    "\n",
    "def relu(X):\n",
    "   return np.maximum(0,X)\n",
    "\n",
    "h2 = relu(h_2)\n",
    "print(h2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <span style=\"color:red\">**Question 1.4.**</span> **Given the DNN architecture below for regression task. This DNN has three layers, assuming all bias terms are zero. The input layer has dimension $M$, the output layer has dimension $K$, the hidden layer has two nodes and uses *sigmoid* activation function. Assume the weight goes from $x_m$ to $h_i$ is $w^h_{im}$, and the weight goes from $h_i$ to $y_k$ is $w^o_{ki}$. Given a single training input $(x,y)$ where $y=(y_1, \\ldots,y_K)$ is a vector of $K$ dimension. Answer the following questions in regarding to back propagation procedure for this network:**\n",
    "\n",
    "<img src='Figures/dnn_reg1.png' width=230>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(a)**</span> What are the expressions to compute forward propagation for $h_1$, $h_2$ and $\\hat{y}_k$?\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 point]</span></div>\n",
    "h_1 = sum((w_1m)^h) * x_m, where 1<=m<=M\n",
    "\n",
    "h1 = sigmoid(h_1) = 1/(1+exp(-h_1))\n",
    "\n",
    "h_2= sum((w_2m)^h) * x_m, where 1<=m<=M\n",
    "\n",
    "h2 = sigmoid(h_2) = 1/(1+exp(-h_2))\n",
    "\n",
    "y_k= sum((w_k1)^o) * h1 + sum((w_k2)^o) * h2, where 1<=k<=K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(b)**</span> **What are the stochastic gradient descent (SGD) update equation for $w^o_{11}$ and $w^o_{k1}$ in general for $k=1,\\ldots,K$ (show your derivation) in case the mini-batch includes only $x= [x_1,...,x_M]$?**\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2.5 points]</span></div>\n",
    "[Note that the same d/dx derivative sign is used even though it is partial derivative for jacobian matrix]\n",
    "\n",
    "\n",
    "The partial derivation shown by: d/dw_11^o (J_t(w))\n",
    "\n",
    "= d/dw_11^o(J_t(w))\n",
    "\n",
    "= (dJ_t/dy_k) * dy_k/dw_11^o\n",
    "\n",
    "= (dJ_t/dy_k) * h1\n",
    "\n",
    "So, Gradient descent update: w_11^o <- w_11^o ‚Äì n * (y_k ‚Äì y) * h1\n",
    "\n",
    "Where n is the learning rate, h1 is the activation input and middle term is error back propogated\n",
    "\n",
    "\n",
    "For w_k1^o in general, Goal is to minimize ùêΩ_t(w) = (1/2) * (y_k - y)^2\n",
    "\n",
    "The partial derivation shown by: d/dw_k1^o (J_t(w))\n",
    "\n",
    "= d/dw_k1^o(J_t(w))\n",
    "\n",
    "= (dJ_t/dy_k) * dy_k/dw_k1^o\n",
    "\n",
    "= (dJ_t/dy_k) * h1\n",
    "\n",
    "So, Gradient descent update: w_k1^o <- w_k1^o ‚Äì n * (y_k ‚Äì y) * h1\n",
    "Where n is the learning rate, h1 is the activation input and middle term is error back propogated\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(c)**</span> **What are the stochastic gradient descent (SGD) update equation for $w^h_{11}$ and $w^h_{1m}$ in general for $m=1, \\ldots, M$ (show your derivation) in case the mini-batch includes only $x= [x_1,...,x_M]$?** \n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2.5 points]</span></div>\n",
    "\n",
    "\n",
    "\n",
    "The partial derivation shown by: d/dw_11^h (J_t(w))\n",
    "\n",
    "\n",
    "= d/dw_11^h(J_t(w))\n",
    "\n",
    "= (dJ_t/dh_1) * (dh_1/dw_11^h), since x1= dh_1/dw_11^h\n",
    "\n",
    "= (dJ_t/dh1) * (dh1/dh_1) * x1, since dh1/dh_1 = h1(1-h1)\n",
    "\n",
    "= (dJ_t/dy_k) * (dy_k/dh1) *  h1(1-h1) * x1, since dy_1/dh1 = w11^o\n",
    "\n",
    "= (dJ_t/dy_k) * w_11^o *  h1(1-h1) * x1, since dJ_t/dy_k = y_k - y\n",
    "\n",
    "= (y_k ‚Äì y) * w_11^o *  h1(1-h1) * x1\n",
    "\n",
    "So, SGD update: ùë§_11^‚Ñé <- ùë§_11^h ‚àí n*‚Ñé1*(1 ‚àí ‚Ñé1)*ùë§_11^o*(ùë¶_k ‚Äì y)* ùë•1\n",
    "\n",
    "\n",
    "For w_1m^h, Goal is to minimise  minimize ùêΩ_t(w) = (1/2) * (y_k - y)^2\n",
    "\n",
    "The partial derivation shown by: d/dw_1m^h (J_t(w))\n",
    "\n",
    "\n",
    "= d/dw_1m^h(J_t(w))\n",
    "\n",
    "= (dJ_t/dh_1) * (dh_1/dw_1m^h), since x_m = dh_1/dw_1m^h\n",
    "\n",
    "= (dJ_t/dh1) * (dh1/dh_1) * x_m, since dh1/dh_1 = h1(1-h1)\n",
    "\n",
    "= (dJ_t/dy_k) * (dy_k/dh1) *  h1(1-h1) * x_m, since dy_k/dh1 = w_k1^o\n",
    "\n",
    "= (dJ_t/dy_k) * w_k1^o *  h1(1-h1) * x_m, since dJ_t/dy_k = y_k - y\n",
    "\n",
    "= (y_k ‚Äì y) * w_k1^o *  h1(1-h1) * x_m\n",
    "\n",
    "So, SGD update: ùë§_1m^‚Ñé <- ùë§_1m^‚Ñé ‚àí n * ‚Ñé1(1 ‚àí ‚Ñé1) * ùë§_k1^o * (ùë¶_k ‚Äì y) * ùë•_m\n",
    "\n",
    "Where n is the learning rate, x_m is the activation input and middle term is error back propogated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**(d)**</span> **What are the stochastic gradient descent (SGD) update equation for $w^h_{im}$ in general for $i=1,2$ and $m=1, \\ldots, M$ (show your derivation) in case the mini-batch includes only $x= [x_1,...,x_M]$?**\n",
    "<div style=\"text-align: right\"><span style=\"color:red\">[2 points]</span></div>\n",
    "\n",
    "\n",
    "\n",
    "For w_im^h, Goal is to minimise minimize ùêΩ_t(w) = (1/2) * (y_k - y)^2\n",
    "\n",
    "The partial derivation shown by: d/dw_im^h (J_t(w))\n",
    "\n",
    "= d/dw_im^h(J_t(w))\n",
    "\n",
    "= (dJ_t/dh_i) * (dh_i/dw_im^h), since x_m = dh_i/dw_im^h\n",
    "\n",
    "= (dJ_t/dhi) * (dhi/dh_i) * x_m, since dhi/dh_i = hi(1-hi)\n",
    "\n",
    "= (dJ_t/dy_k) * (dy_k/dhi) * hi(1-hi) * x_m, since dy_k/dhi = w_ki^o\n",
    "\n",
    "= (dJ_t/dy_k) * w_ki^o * hi(1-hi) * x_m, since dJ_t/dy_k = y_k - y\n",
    "\n",
    "= (y_k ‚Äì y) * w_ki^o * hi(1-hi) * x_m\n",
    "\n",
    "So, SGD update: ùë§_im^‚Ñé <- ùë§_im^‚Ñé ‚àí n * ‚Ñéi(1 ‚àí ‚Ñéi) * ùë§_ki^o * (ùë¶_k ‚Äì y) * ùë•_m\n",
    "\n",
    "Where n is the learning rate, x_m is the activation input and middle term is error back propogated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
