{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Set random seeds</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with importing tensorflow and numpy and setting random seeds for TF and numpy. You can use any seeds you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.random.set_seed(6789)\n",
    "np.random.seed(6789)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: Download and preprocess the data</span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we use for this assignment is a question classification dataset for which the train set consists of $5,500$ questions belonging to 6 coarse question categories including:\n",
    "- abbreviation (ABBR), \n",
    "- entity (ENTY), \n",
    "- description (DESC), \n",
    "- human (HUM), \n",
    "- location (LOC) and \n",
    "- numeric (NUM).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data is an inital and important step in any machine learning or deep learning projects. The following *DataManager* class helps you to download data and preprocess data for the later steps of a deep learning project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import collections\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "class DataManager:\n",
    "    def __init__(self, verbose=True, maxlen= 50, random_state=6789):\n",
    "        self.verbose = verbose\n",
    "        self.max_sentence_len = 0\n",
    "        self.str_questions = list()\n",
    "        self.str_labels = list()\n",
    "        self.numeral_labels = list()\n",
    "        self.maxlen = maxlen\n",
    "        self.numeral_data = list()\n",
    "        self.random_state = random_state\n",
    "        self.random = np.random.RandomState(random_state)\n",
    "        \n",
    "    @staticmethod\n",
    "    def maybe_download(dir_name, file_name, url, verbose= True):\n",
    "        if not os.path.exists(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "        if not os.path.exists(os.path.join(dir_name, file_name)):\n",
    "            urlretrieve(url + file_name, os.path.join(dir_name, file_name))\n",
    "        if verbose:\n",
    "            print(\"Downloaded successfully {}\".format(file_name))\n",
    "    \n",
    "    def read_data(self, dir_name, file_names):\n",
    "        for file_name in file_names:\n",
    "            file_path= os.path.join(dir_name, file_name)\n",
    "            self.str_questions= list(); self.str_labels= list()\n",
    "            with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "                for row in f:\n",
    "                    row_str= row.split(\":\")\n",
    "                    label, question= row_str[0], row_str[1]\n",
    "                    question= question.lower()\n",
    "                    self.str_labels.append(label)\n",
    "                    self.str_questions.append(question[0:-1])\n",
    "                    if self.max_sentence_len < len(self.str_questions[-1]):\n",
    "                        self.max_sentence_len= len(self.str_questions[-1])\n",
    "         \n",
    "        # turns labels into numbers\n",
    "        le= preprocessing.LabelEncoder()\n",
    "        le.fit(self.str_labels)\n",
    "        self.numeral_labels = np.array(le.transform(self.str_labels))\n",
    "        self.str_classes= le.classes_\n",
    "        self.num_classes= len(self.str_classes)\n",
    "        if self.verbose:\n",
    "            print(\"\\nSample questions... \\n\")\n",
    "            print(self.str_questions[0:5])\n",
    "            print(\"Labels {}\\n\\n\".format(self.str_classes))\n",
    "    \n",
    "    def manipulate_data(self):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(self.str_questions)\n",
    "        self.numeral_data = tokenizer.texts_to_sequences(self.str_questions)\n",
    "        self.numeral_data = tf.keras.preprocessing.sequence.pad_sequences(self.numeral_data, padding='post', truncating= 'post', maxlen= self.maxlen)\n",
    "        self.word2idx = tokenizer.word_index\n",
    "        self.word2idx = {k:v for k,v in self.word2idx.items()}\n",
    "        self.idx2word = {v:k for k,v in self.word2idx.items()}\n",
    "        self.vocab_size = len(self.word2idx)\n",
    "    \n",
    "    def train_valid_split(self, train_ratio=0.9):\n",
    "        idxs = np.random.permutation(np.arange(len(self.str_questions)))\n",
    "        train_size = int(train_ratio*len(idxs)) +1\n",
    "        self.train_str_questions, self.valid_str_questions = self.str_questions[0:train_size], self.str_questions[train_size:]\n",
    "        self.train_numeral_data, self.valid_numeral_data = self.numeral_data[0:train_size], self.numeral_data[train_size:]\n",
    "        self.train_numeral_labels, self.valid_numeral_labels = self.numeral_labels[0:train_size], self.numeral_labels[train_size:]\n",
    "        self.tf_train_set = tf.data.Dataset.from_tensor_slices((self.train_numeral_data, self.train_numeral_labels))\n",
    "        self.tf_valid_set = tf.data.Dataset.from_tensor_slices((self.valid_numeral_data, self.valid_numeral_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Sample questions... \n",
      "\n",
      "['manner how did serfdom develop in and then leave russia ?', 'cremat what films featured the character popeye doyle ?', \"manner how can i find a list of celebrities ' real names ?\", 'animal what fowl grabs the spotlight after the chinese year of the monkey ?', 'exp what is the full form of .com ?']\n",
      "Labels ['ABBR' 'DESC' 'ENTY' 'HUM' 'LOC' 'NUM']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Loading data...')\n",
    "\n",
    "dm = DataManager(maxlen=100)\n",
    "dm.read_data(\"Data/\", [\"train_set.label\"])   # read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.manipulate_data()\n",
    "dm.train_valid_split(train_ratio=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You now have a data manager, named *dm* containing the training and validiation sets in both text and numeric forms. Your task is to play around and read this code to figure out the meanings of some important attributes that will be used in the next parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**What is the purpose of `self.train_str_questions` and `self.train_numeral_labels`? Write your code to print out the first five questions with labels in the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of self.train_str_questions is to store all the quentions present in the training dataset by reading the data as above and there are 5,500 questions in the dataset belonging to 6 coarse question categories. Therefore, self.train_str_questions is the extraction of questions from the training data into the list.\n",
    "\n",
    "\n",
    "The purpose of self.train_numeral_labels is to store the corresponding labels for the above questions in train_str_questions list. The benifit of self.train_numeral_labels list is that it would have the labels converted into numerical form from the original string format which would make it easier in the next stages to classify the questions values into corresponding labels. \n",
    "\n",
    "Below, top-5 string questions in the list of training dataset with their corresponding labels are printed in the form of item.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1: manner how did serfdom develop in and then leave russia ?)\n",
      "(2: cremat what films featured the character popeye doyle ?)\n",
      "(1: manner how can i find a list of celebrities ' real names ?)\n",
      "(2: animal what fowl grabs the spotlight after the chinese year of the monkey ?)\n",
      "(0: exp what is the full form of .com ?)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dm.train_str_questions)):\n",
    "    for j in range(len(dm.train_numeral_labels)):\n",
    "        if i==j:\n",
    "            print(\"({:d}: {:s})\".format(dm.train_numeral_labels[j],dm.train_str_questions[i]))\n",
    "        \n",
    "        if i>=5 or j>=5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**What is the purpose of `self.train_numeral_data`? Write your code to print out the first five questions in the numeric format with labels in the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of self.train_numeral_data is to storing the questions in from the training dataset into numerical form of 100 dimension array. This would help converting the texts (string) of the questions into numerical array form. Therefore, self.train_numeral_data is useful to make text analysis of the given text (questions) with numerical values allocated for each question.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1: [  29    8   19 3497 2219    5   16  433  814  990    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0])\n",
      "(2: [  32    2  815  619    1  148 1255 3498    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0])\n",
      "(1: [  29    8   37   35   67    6  484    4 1614   88  334  186    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0])\n",
      "(2: [  46    2 3499 3500    1 3501  156    1  485   68    4    1 1615    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0])\n",
      "(0: [ 81   2   3   1 486 300   4 396   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(dm.train_numeral_data)):\n",
    "    for j in range(len(dm.train_numeral_labels)):\n",
    "        if i==j:\n",
    "            print(\"({:d}: {})\".format(dm.train_numeral_labels[j],dm.train_numeral_data[i]))\n",
    "        \n",
    "        if i>=5 or j>=5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**What is the purpose of two dictionaries: `self.word2idx` and `self.idx2word`? Write your code to print out the first five key-value pairs of those dictionaries.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dictionary self.word2idx defines the words and their corresponding labels into key-vlue form. The dictionary contains one pair for each word and the pair consisting of word and its corresponding key. So, based on the key, dictionary can find its corresponding word which is the pair element.\n",
    "\n",
    "The dictionary self.idx2word contains the same structure as self.word2idx dictionary but it would store labels for each word and based on word as the key, the labels can be retained. \n",
    "\n",
    "This will be useful for analysing the sentence or string of words which can be classified into words and dictionary would find the labels based on corresponding words which can be used to calculate the features of the sentence or string overall.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, the)\n",
      "(2, what)\n",
      "(3, is)\n",
      "(4, of)\n",
      "(5, in)\n",
      "-----------\n",
      "(the, 1)\n",
      "(what, 2)\n",
      "(is, 3)\n",
      "(of, 4)\n",
      "(in, 5)\n"
     ]
    }
   ],
   "source": [
    "for i, (key,word) in enumerate(dm.idx2word.items()):\n",
    "        print(\"({:d}, {:s})\".format(key,word))\n",
    "        if i >= 4:\n",
    "            break\n",
    "            \n",
    "print(\"-----------\")\n",
    "    \n",
    "for i, (word,key) in enumerate(dm.word2idx.items()):\n",
    "        print(\"({:s}, {:d})\".format(word,key))\n",
    "        if i >= 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**What is the purpose of `self.tf_train_set`? Write your code to print out the first five items of `self.tf_train_set`.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self.tf_train_set takes the slices of an array of self.train_numeral_data (which is the numerical conversion of questions in string form).\n",
    "\n",
    "With the help of tf.data.Dataset.from_tensor_slices() method, we can get the slices of an array in the form of objects and we can allocate the labels into another tensor object. The method from_tensor_slices() accepts individual (or multiple) Numpy (or Tensors) objects. In case multiple objects are inputted, it passes them as tuple and make sure that all the objects have same size in zeroth dimension. Therefore, self.tf_train_set would have the numerical form of question in the training dataset and the corresponding labels for each question in the form of object which would be retained in the tensor form. \n",
    "\n",
    "The training dataset in such form would be useful while fitting the model as it becomes easy to enter the training dataset containing both questions and labels in the form of objects in numeric datatype. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  29    8   19 3497 2219    5   16  433  814  990    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  32    2  815  619    1  148 1255 3498    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  29    8   37   35   67    6  484    4 1614   88  334  186    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(1, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  46    2 3499 3500    1 3501  156    1  485   68    4    1 1615    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[ 81   2   3   1 486 300   4 396   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0], shape=(100,), dtype=int32) tf.Tensor(0, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for i, (x,y) in enumerate(dm.tf_train_set):\n",
    "    print(x,y)\n",
    "    \n",
    "    if i>=4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**What is the purpose of `self.tf_valid_set`? Write your code to print out the first five items of `self.tf_valid_set`.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The self.tf_valid_set takes the slices of an array of self.valid_numeral_data (which is the numerical conversion of questions in string form).\n",
    "\n",
    "In the same format as the training dataset, tf.data.Dataset.from_tensor_slices() method converts the validation dataset into form of objects carrying the numerical form of question and corresponding labels for each question. \n",
    "\n",
    "While fitting the model, the argument for validation dataset can be given as tensor form as produced here which would make it in the compatible form to access the data along with their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[  38   12  279    1   33 2178    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(3, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  27    2 6443  584   27   55    1 6444  158 6445    1 6446  158   69\n",
      " 6447    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  30    2  765   30    4 6448   63 6449 3299 6450 6451   16 6452  106\n",
      "  978    5    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(4, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  74    2   74  601  685   14   92   69 6453    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(2, shape=(), dtype=int64)\n",
      "tf.Tensor(\n",
      "[  60    2    3    6  275 2744 2179   70    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0], shape=(100,), dtype=int32) tf.Tensor(2, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for i, (x,y) in enumerate(dm.tf_valid_set):\n",
    "    print(x,y)\n",
    "    \n",
    "    if i>=4:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Using Word2Vect to transform texts to vectors </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will be assessed on how to use a pretrained Word2Vect model for realizing a machine learning task. Basically, you will use this pretrained Word2Vect to transform the questions in the above dataset stored in the *data manager object dm* to numeric form for training a Support Vector Machine in sckit-learn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Write code to download the pretrained model *glove-wiki-gigaword-100*. Note that this model transforms a word in its dictionary to a $100$ dimensional vector.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2vect = api.load(\"glove-wiki-gigaword-100\") #Downloading the pre-trained model using \"api\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Write code for the function get_word_vector(word, model) used to transform a word to a vector usingthe pretrained Word2Vect model model. Note that for a word not in the vocabulary of our word2vect,you need to return a vector $0$ with 100 dimensions.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word, model):\n",
    "    #Transforming a word to vector with Word2Vect model\n",
    "    try:\n",
    "        vector = model.get_vector(word) \n",
    "    except: #word not in the vocabulary\n",
    "        vector = np.zeros([model.vector_size]) \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**Write the code for the function *get_sentence_vector(sentence, important_score=None, model= None)*. Note that this function will transform a sentence to a 100-dimensional vector using the pretrained model *model*. In addition, the list *important_score* which has the same length as the *sentence* specifies the important scores of the words in the sentence. In your code, you first need to apply *softmax* function over *important_score* to obtain the important weight *important_weight* which forms a probability over the words of the sentence. Furthermore, the final vector of the sentence will be weighted sum of the individual vectors for words and the weights in *important_weight*.**\n",
    "- $final\\_vector= important\\_weight[1]\\times v[1] + important\\_weight[2]\\times v[2] + ...+ important\\_weight[L]\\times v[L]$ where $L$ is the length of the sentence and $v[i]$ is the vector of the word $i-th$ in this sentence.\n",
    "\n",
    "**Note that if *important_score=None* is set by default, your function should return the average of all representation vectors corresponding to set *important_score=[1,1,...,1]*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, important_score=None, model= None):\n",
    "    #Transforming a sentence to a 100 dimensional vector\n",
    "    #Here the important_score is calculated using the index of the token \n",
    "    #or word into sentence where the index of the last word will be the \n",
    "    #important_score value of the first word in the sentence, second\n",
    "    #last word's index to important_score value of second word and so on    \n",
    "    \n",
    "    token = sentence.split() #Splitting the sentence into tokens\n",
    "    \n",
    "    important_score_values = [token for token in range(len(token))]#Allocating the index values\n",
    "    important_score_values = important_score_values[::-1] #Reversing the order \n",
    "    \n",
    "    \n",
    "    \n",
    "    if important_score == None:\n",
    "        important_score = np.ones([len(token)]) #In case none is received, it will be equal \n",
    "        \n",
    "    else:\n",
    "        important_score = important_score_values  \n",
    "    \n",
    "    imp_exp = np.exp(important_score - np.max(important_score))\n",
    "    important_weight = imp_exp / imp_exp.sum(axis=0) #Activation function weight allocation\n",
    "    \n",
    "    vec = [get_word_vector(t, model) for t in token] #Retrieving the word into numeric vector\n",
    "    \n",
    "    vector = []\n",
    "    for i in range(len(important_weight)):\n",
    "        vector += [important_weight[i] * (np.array(vec[i]))] #Calculation for the sentence\n",
    "    \n",
    "    if len(vector) > 0:\n",
    "        vector = np.asarray(vector).sum(axis = 0) \n",
    "    return vector\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**Write code to transform the training questions in *dm.train_str_questions* to feature vectors. Note that after running the following cell, you must have $X\\_train$ which is an numpy array of the feature vectors and $y\\_train$ which is an array of numeric labels (*Hint: dm.train_numeral_labels*). You can add more lines to the following cell if necessary. In addition, you should decide the *important_score* by yourself. For example, you might reckon that the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform training set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform training set to feature vectors...\")\n",
    "\n",
    "X_train= [] \n",
    "y_train= dm.train_numeral_labels #Allocating the labels into training array\n",
    "\n",
    "for line in dm.train_str_questions:\n",
    "    vector = get_sentence_vector(line,100,word2vect) #Converting sentences into vec\n",
    "\n",
    "    if len(vector)>0:\n",
    "        X_train += [vector] #Building the training dataset with all sentences vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**Write code to transform the training questions in *dm.valid_str_questions* to feature vectors. Note that after running the following cell, you must have $X\\_valid$ which is an numpy array of the feature vectors and $y\\_valid$ which is an array of numeric labels (*Hint: dm.valid_numeral_labels*). You can add more lines to the following cell if necessary. In addition, you should decide the *important_score* by yourself. For example, you might reckon that the 1st score is 1, the 2nd score is decayed by 0.9, the 3rd is decayed by 0.9, and so on.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform valid set to feature vectors...\n"
     ]
    }
   ],
   "source": [
    "print(\"Transform valid set to feature vectors...\")\n",
    "\n",
    "X_valid= []\n",
    "y_valid= dm.valid_numeral_labels #Allocating the labels into validation array\n",
    "\n",
    "\n",
    "\n",
    "for line in dm.valid_str_questions:\n",
    "    vector = get_sentence_vector(line,important_score,word2vect) #Converting sentences into vec\n",
    "\n",
    "    if len(vector)>0:\n",
    "        X_valid += [vector] #Building the validation dataset with all sentences vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**It is now to use *MinMaxScaler(feature_range=(-1,1))* in sckit-learn to scale both training and valid sets to the range $(-1,1)$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train) #Scalling the training dataset values into range (-1,1)\n",
    "scaler.fit(X_valid)\n",
    "X_valid = scaler.transform(X_valid) #Scalling the validation dataset values into range (-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**Declare a support vector machine (the class *SVC*  in sckit-learn) with RBF kernel, $C=1$, $gamma= 2^{-3}$ and fit on the training set.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1, gamma=0.125)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(C=1, kernel = 'rbf',gamma = 2**(-3)) #the class SVC\n",
    "svm.fit(X_train, y_train) #fitting svm on X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**Finally, we use the trained *svm* to evaluate on the valid set $X\\_valid$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9660550458715597\n"
     ]
    }
   ],
   "source": [
    "y_valid_pred= svm.predict(X_valid) #predicting the values on validation dataset for testing\n",
    "acc = accuracy_score(y_valid, y_valid_pred)#Computing the accuracy on validation actual values \n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: Text CNN for sequence modeling and neural embedding </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**In what follows, you are required to complete the code for Text CNN for sentence classification. The paper of Text CNN can be found at this [link](https://www.aclweb.org/anthology/D14-1181.pdf). Here is the description of the Text CNN you need to construct.**\n",
    "- There are three attributes (properties or instance variables): *embed_size, state_size, data_manager*.\n",
    "  - `embed_size`: the dimension of the vector space for which the words are embedded to using the embedding matrix.\n",
    "  - `state_size`: the number of filters used in *Conv1D* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D)).\n",
    "  - `data_manager`: the data manager to store information of the dataset.\n",
    "- The detail of the computational process is as follows:\n",
    "  - Given input $x$, we embed $x$ using the embedding matrix to obtain an $3D$ tensor $[batch\\_size \\times vocab\\_size \\times embed\\_size]$ as $h$.\n",
    "  - We feed $h$ to three Convd 1D layers, each of which has $state\\_size$ filters, padding=same, activation= relu, and $kernel\\_size= 3, 5, 7$ respectively to obtain $h1, h2, h3$. Note that each $h1, h2, h3$ is a 3D tensor with the shape $[batch\\_size \\times output\\_size \\times state\\_size]$.\n",
    "  - We then apply *GlobalMaxPool1D()* (reference [here](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D)) over $h1, h2, h3$ to obtain 2D tensors stored in $h1, h2, h3$ again.\n",
    "  - We then concatenate three 2D tensors $h1, h2, h3$ to obtain $h$. Note that you need to specify the axis to concatenate.\n",
    "  - We finally build up one dense layer on the top of $h$ for classification.\n",
    "  \n",
    "  <div style=\"text-align: right\"><span style=\"color:red\"></span></div>\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN:\n",
    "    def __init__(self, embed_size= 128, state_size=16, data_manager=None):\n",
    "        self.data_manager = data_manager\n",
    "        self.embed_size = embed_size\n",
    "        self.state_size = state_size\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.data_manager.vocab_size +1, self.embed_size)(x)\n",
    "        h1 = tf.keras.layers.Conv1D(filters = self.state_size, padding = 'same', kernel_size=3, activation= 'relu')(h)#1D convolutional layer\n",
    "        h2 = tf.keras.layers.Conv1D(filters = self.state_size, padding = 'same', kernel_size=5, activation= 'relu')(h)#1D convolutional layer\n",
    "        h3 = tf.keras.layers.Conv1D(filters = self.state_size, padding = 'same', kernel_size=7, activation= 'relu')(h)#1D convolutional layer\n",
    "        h1 = tf.keras.layers.GlobalMaxPool1D()(h1)#1D Global max pooling layer\n",
    "        h2 = tf.keras.layers.GlobalMaxPool1D()(h2)#1D Global max pooling layer\n",
    "        h3 = tf.keras.layers.GlobalMaxPool1D()(h3)#1D Global max pooling layer\n",
    "        h = tf.keras.layers.concatenate([h1,h2,h3],axis=-1,name=\"concatenate\")#Concatenation of h1, h2 and h3 layers\n",
    "        h = tf.keras.layers.Dense(self.data_manager.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h) \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Here is the code to test TextCNN above. You can observe that TextCNN outperforms the traditional approach SVM + Word2Vect for this task. The reason is that TextCNN enables us to automatically learn the feature that fits to the task. This makes deep learning different from hand-crafted feature approaches. Complete the code to test the model. Note that when compiling the model, you can use the Adam optimizer.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 8s 154ms/step - loss: 1.4240 - accuracy: 0.6773 - val_loss: 0.9021 - val_accuracy: 0.8592\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 5s 105ms/step - loss: 0.4350 - accuracy: 0.9141 - val_loss: 0.2352 - val_accuracy: 0.9427\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 6s 109ms/step - loss: 0.1348 - accuracy: 0.9697 - val_loss: 0.1216 - val_accuracy: 0.9661\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 5s 103ms/step - loss: 0.0642 - accuracy: 0.9890 - val_loss: 0.0834 - val_accuracy: 0.9761\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 5s 104ms/step - loss: 0.0347 - accuracy: 0.9966 - val_loss: 0.0683 - val_accuracy: 0.9789\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 5s 102ms/step - loss: 0.0204 - accuracy: 0.9976 - val_loss: 0.0593 - val_accuracy: 0.9794\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 5s 102ms/step - loss: 0.0128 - accuracy: 0.9994 - val_loss: 0.0532 - val_accuracy: 0.9784\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 6s 113ms/step - loss: 0.0084 - accuracy: 0.9997 - val_loss: 0.0500 - val_accuracy: 0.9784\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 6s 113ms/step - loss: 0.0059 - accuracy: 0.9997 - val_loss: 0.0488 - val_accuracy: 0.9784\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 6s 107ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.0479 - val_accuracy: 0.9784\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 5s 102ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 0.9784\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 5s 100ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0458 - val_accuracy: 0.9798\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 6s 106ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.0452 - val_accuracy: 0.9794\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 5s 95ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0448 - val_accuracy: 0.9794\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 5s 93ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0447 - val_accuracy: 0.9794\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 5s 98ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9794\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 5s 98ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9794\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 5s 100ms/step - loss: 9.6192e-04 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 0.9794\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 5s 102ms/step - loss: 8.4288e-04 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9794\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 6s 107ms/step - loss: 7.4440e-04 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 0.9794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22322b6b748>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cnn = TextCNN(data_manager=dm)\n",
    "text_cnn.build()\n",
    "text_cnn.compile_model(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy']) #compile the model\n",
    "text_cnn.fit(dm.tf_train_set.batch(64), validation_data=dm.tf_valid_set.batch(64), epochs=20) #train the model on 20 epochs\n",
    "#Proving the better result than hand-crafted approch with validation accuracy of 97.94%\n",
    "#as compared to 96.60% by hand-crafted SVM model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: RNNs for sequence modeling and neural embedding </span>\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">4.1. One-directional RNNs for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**In this part, you need to construct an RNN to learn from the dataset of interest. Basically, you are required first to construct the class UniRNN (Uni-directional RNN) with the following requirements:**\n",
    "- Attribute `data_manager (self.data_manager)`: specifies the data manager used to store data for the model.\n",
    "- Attribute `cell_type (self.cell_type)`: can receive three values including `basic_rnn`, `gru`, and `lstm` which specifies the memory cells formed a hidden layer.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes from the second hidden layers of memory cells. For example, $embed\\_size =128$ and $state\\_sizes = [64, 64]$ means that you have three hidden layers in your network with hidden sizes of $128, 64$ and $64$ respectively.\n",
    "\n",
    "**Note that when declaring an embedding layer for the network, you need to set *mask_zero=True* so that the padding zeros in the sentences will be masked and ignored. This helps to have variable length RNNs. For more detail, you can refer to this [link](https://www.tensorflow.org/guide/keras/masking_and_padding).**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UniRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1 \n",
    "        \n",
    "    #return the correspoding memory cell\n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if cell_type=='gru':\n",
    "            return tf.keras.layers.GRU(state_size, return_sequences=return_sequences) #GRU memory cell\n",
    "        elif cell_type== 'lstm':\n",
    "            return tf.keras.layers.LSTM(state_size, return_sequences=return_sequences)#LSTM memory cell\n",
    "        else:\n",
    "            return tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences) #Basic RNN memory cell\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero= True)(x)#Embedding layer\n",
    "        num_layers = len(self.state_sizes) #number of layers\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i==num_layers-1:\n",
    "                h = UniRNN.get_layer(self.cell_type,self.state_sizes[i],return_sequences= False, activation = 'tanh')(h) #calling get_layer method to run memory cell \n",
    "                                                                                                                         #when the current layer is the last one for the output shape\n",
    "            else:\n",
    "                h = UniRNN.get_layer(self.cell_type,self.state_sizes[i],return_sequences= True, activation = 'tanh')(h) #calling get_layer method to run memory cell \n",
    "                                                                                                                         #when the current layer is not the last one for the output shape\n",
    "           \n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "   \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Run with basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 14s 274ms/step - loss: 0.5842 - accuracy: 0.7961 - val_loss: 0.1501 - val_accuracy: 0.9601\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 9s 170ms/step - loss: 0.2095 - accuracy: 0.9496 - val_loss: 1.0362 - val_accuracy: 0.7239\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 10s 184ms/step - loss: 0.1247 - accuracy: 0.9624 - val_loss: 0.1162 - val_accuracy: 0.9601\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 9s 168ms/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 0.0716 - val_accuracy: 0.9739\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 9s 181ms/step - loss: 0.0572 - accuracy: 0.9872 - val_loss: 0.0722 - val_accuracy: 0.9734\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 10s 185ms/step - loss: 0.0152 - accuracy: 0.9960 - val_loss: 0.0886 - val_accuracy: 0.9757\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 9s 175ms/step - loss: 0.0484 - accuracy: 0.9887 - val_loss: 0.0918 - val_accuracy: 0.9739\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 10s 188ms/step - loss: 0.0065 - accuracy: 0.9979 - val_loss: 0.2659 - val_accuracy: 0.9252\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 11s 209ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.0774 - val_accuracy: 0.9794\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.0454 - accuracy: 0.9905 - val_loss: 0.0833 - val_accuracy: 0.9757\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 11s 204ms/step - loss: 3.0246e-04 - accuracy: 1.0000 - val_loss: 0.0851 - val_accuracy: 0.9771\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 11s 205ms/step - loss: 0.0163 - accuracy: 0.9960 - val_loss: 0.0892 - val_accuracy: 0.9693\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 12s 224ms/step - loss: 0.0145 - accuracy: 0.9951 - val_loss: 0.0909 - val_accuracy: 0.9729\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 11s 217ms/step - loss: 1.4005e-04 - accuracy: 1.0000 - val_loss: 0.0813 - val_accuracy: 0.9771\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 11s 206ms/step - loss: 4.2807e-05 - accuracy: 1.0000 - val_loss: 0.0954 - val_accuracy: 0.9761\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 10s 189ms/step - loss: 8.9321e-06 - accuracy: 1.0000 - val_loss: 0.1262 - val_accuracy: 0.9766\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 10s 197ms/step - loss: 0.0408 - accuracy: 0.9945 - val_loss: 0.1391 - val_accuracy: 0.9757\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 13s 243ms/step - loss: 0.0163 - accuracy: 0.9957 - val_loss: 0.0818 - val_accuracy: 0.9748\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 11s 217ms/step - loss: 0.0055 - accuracy: 0.9988 - val_loss: 0.1169 - val_accuracy: 0.9743\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 11s 218ms/step - loss: 0.0254 - accuracy: 0.9948 - val_loss: 0.1242 - val_accuracy: 0.9743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2232300af98>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN(None, embed_size=128, state_sizes=[128,128], data_manager=dm)#Basic RNN network\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Run with GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 40s 763ms/step - loss: 0.8658 - accuracy: 0.6592 - val_loss: 0.2457 - val_accuracy: 0.9261\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 28s 535ms/step - loss: 0.1349 - accuracy: 0.9615 - val_loss: 0.1314 - val_accuracy: 0.9624\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 24s 455ms/step - loss: 0.0485 - accuracy: 0.9853 - val_loss: 0.0937 - val_accuracy: 0.9711\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 26s 491ms/step - loss: 0.0227 - accuracy: 0.9945 - val_loss: 0.1508 - val_accuracy: 0.9619\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 26s 493ms/step - loss: 0.0176 - accuracy: 0.9966 - val_loss: 0.1479 - val_accuracy: 0.9638\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 27s 511ms/step - loss: 0.0054 - accuracy: 0.9991 - val_loss: 0.1540 - val_accuracy: 0.9693\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 26s 492ms/step - loss: 0.0069 - accuracy: 0.9985 - val_loss: 0.1275 - val_accuracy: 0.9729\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 26s 497ms/step - loss: 2.1326e-04 - accuracy: 1.0000 - val_loss: 0.1522 - val_accuracy: 0.9734\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 26s 503ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.1995 - val_accuracy: 0.9702\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 26s 499ms/step - loss: 2.5081e-04 - accuracy: 1.0000 - val_loss: 0.1767 - val_accuracy: 0.9725\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 26s 504ms/step - loss: 1.5349e-05 - accuracy: 1.0000 - val_loss: 0.1744 - val_accuracy: 0.9739\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 29s 553ms/step - loss: 5.5436e-05 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.9775\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 27s 526ms/step - loss: 2.4787e-06 - accuracy: 1.0000 - val_loss: 0.2204 - val_accuracy: 0.9761\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 28s 542ms/step - loss: 5.6432e-07 - accuracy: 1.0000 - val_loss: 0.2274 - val_accuracy: 0.9771\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 26s 510ms/step - loss: 1.8821e-07 - accuracy: 1.0000 - val_loss: 0.2393 - val_accuracy: 0.9780\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 26s 502ms/step - loss: 6.9719e-08 - accuracy: 1.0000 - val_loss: 0.2437 - val_accuracy: 0.9775\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 27s 520ms/step - loss: 3.8443e-08 - accuracy: 1.0000 - val_loss: 0.2471 - val_accuracy: 0.9771\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 26s 502ms/step - loss: 2.6941e-08 - accuracy: 1.0000 - val_loss: 0.2499 - val_accuracy: 0.9771\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 26s 501ms/step - loss: 2.0102e-08 - accuracy: 1.0000 - val_loss: 0.2521 - val_accuracy: 0.9771\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 27s 511ms/step - loss: 1.6411e-08 - accuracy: 1.0000 - val_loss: 0.2538 - val_accuracy: 0.9775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22338733630>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN('gru', embed_size=128, state_sizes=[128,128], data_manager=dm)#GRU cell\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Run with LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 41s 779ms/step - loss: 0.7405 - accuracy: 0.7375 - val_loss: 0.2035 - val_accuracy: 0.9399\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 36s 701ms/step - loss: 0.1328 - accuracy: 0.9603 - val_loss: 0.1126 - val_accuracy: 0.9642\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 35s 677ms/step - loss: 0.0681 - accuracy: 0.9765 - val_loss: 0.0915 - val_accuracy: 0.9665\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 34s 653ms/step - loss: 0.0416 - accuracy: 0.9869 - val_loss: 0.0935 - val_accuracy: 0.9697\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 37s 712ms/step - loss: 0.0230 - accuracy: 0.9936 - val_loss: 0.1183 - val_accuracy: 0.9706\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 36s 683ms/step - loss: 0.0147 - accuracy: 0.9969 - val_loss: 0.1383 - val_accuracy: 0.9706\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 36s 688ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.1111 - val_accuracy: 0.9748\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 33s 628ms/step - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.1211 - val_accuracy: 0.9766\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 38s 726ms/step - loss: 0.0036 - accuracy: 0.9994 - val_loss: 0.1300 - val_accuracy: 0.9789\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 43s 820ms/step - loss: 2.0097e-04 - accuracy: 1.0000 - val_loss: 0.1588 - val_accuracy: 0.9766\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 36s 700ms/step - loss: 0.0053 - accuracy: 0.9994 - val_loss: 0.1877 - val_accuracy: 0.9752\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 32s 621ms/step - loss: 1.0699e-05 - accuracy: 1.0000 - val_loss: 0.1936 - val_accuracy: 0.9748\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 33s 630ms/step - loss: 5.1350e-06 - accuracy: 1.0000 - val_loss: 0.2100 - val_accuracy: 0.9761\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 34s 647ms/step - loss: 2.0731e-06 - accuracy: 1.0000 - val_loss: 0.2482 - val_accuracy: 0.9729\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 33s 641ms/step - loss: 3.3601e-05 - accuracy: 1.0000 - val_loss: 0.2730 - val_accuracy: 0.9748\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 33s 643ms/step - loss: 1.9857e-07 - accuracy: 1.0000 - val_loss: 0.2756 - val_accuracy: 0.9748\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 32s 611ms/step - loss: 1.2350e-07 - accuracy: 1.0000 - val_loss: 0.2796 - val_accuracy: 0.9739\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 32s 613ms/step - loss: 7.0046e-08 - accuracy: 1.0000 - val_loss: 0.2752 - val_accuracy: 0.9748\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 33s 627ms/step - loss: 4.0772e-08 - accuracy: 1.0000 - val_loss: 0.2894 - val_accuracy: 0.9743\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 36s 691ms/step - loss: 2.9525e-08 - accuracy: 1.0000 - val_loss: 0.2931 - val_accuracy: 0.9743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x223509f7438>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_rnn = UniRNN('lstm', embed_size=128, state_sizes=[128,128], data_manager=dm)#LSTM cell\n",
    "uni_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "uni_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "uni_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Give your own comments about the performance of three memory cells for the dataset of interest as well as what happening during the training process of each cell. Note that there are not right or wrong comments and your comments rely on the status of your training. In addition, some comments and hypothesized assessments of what and why are occurring are useful to obtain the highest score for this question.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UniRNN() is one directional network built with three memory cell types which are Simple RNN, GRU and LSTM. In terms of validation accuracy, all three models for their memory cells are performing quite similar. Applying both basic and LSTM memory cells for RNN gives the same validation accuracy of 97.43%, while GRU memory cell for RNN has a validation accuracy of 97.75% which is slight better performance than the other two memory cells.  \n",
    "\n",
    "Simple RNN memory is only capable of saving the information on previous data in the hidden state. When words get transformed into machine-readable vectors using simple RNN, it gets processed one by one sequence of vectors and this way, it passes the previous hidden state to the next step of the sequence. And hidden state acts as the neural network memory which is the only information RNN can store which gets overwritten at each step. Hence, not being able to store information further makes it model decreasing its capability to learn previous aspects. But the computation process of simple RNN includes the tanh activation function.\n",
    "\n",
    "The tanh activation is used to help regulate the values flowing through the network which makes sure the values stay between -1 and 1. Activation function helps saving the model from running into gradient exploding issues which is the reason simple RNN succeeds in providing decent results in the end. Finally, simple RNN has very few operations internally but works pretty well given the right circumstances and here given the short sequences, it performs good.\n",
    "\n",
    "On the other hand, LSTM can choose which information is relevant to remember or forget during sequence processing because of its memory cell structure. LSTM also has access to forget layer additionally which can remove the information if not relevant to remember.\n",
    "\n",
    "Finally, the reason behind GRU performing better is that GRU is the newer generation of Recurrent Neural networks with upgraded fetures. Instead of using the cell state, it has the hidden state to transfer information. It also only has two gates, a reset gate and update gate where update gate gets to decides what information to throw away and what new information to add and reset gate gets to decide how much past information to forget. Therefore, GRU is having features which allows to access past information in more flexible and speedier way. So, GRU's fewer tensor operations makes them a little speedier to train than LSTMs which makes the main difference between their performances.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">4.2. Bi-directional RNNs for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**In what follow, you will investigate BiRNN. The task is similar to Part 4.1 but you need to write the code for an BiRNN. Note that the function *get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh')* has to return the hidden layer with bidirectional memory cells (e.g., Basic RNN, GRU, and LSTM cells).**\n",
    "\n",
    "**Complete the code of the class *BiRNN*. Note that for the embedding layer you need to set *mask_zero=True*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiRNN:\n",
    "    def __init__(self, cell_type= 'gru', embed_size= 128, state_sizes= [128, 64], data_manager= None):\n",
    "        self.cell_type = cell_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_size = embed_size\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = self.data_manager.vocab_size +1\n",
    "        \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', state_size= 128, return_sequences= False, activation = 'tanh'):\n",
    "        if cell_type=='gru':\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.GRU(state_size, return_sequences=return_sequences)) #GRU cell execution in bi-direction\n",
    "        elif cell_type== 'lstm':\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(state_size, return_sequences=return_sequences)) #LSTM cell execution in bi-direction\n",
    "        else:\n",
    "            return tf.keras.layers.Bidirectional(tf.keras.layers.SimpleRNN(state_size, return_sequences=return_sequences)) #Simple RNN cell execution in bi-direction\n",
    "    \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        \n",
    "        h = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero= True)(x)#Embedding layer\n",
    "        num_layers = len(self.state_sizes) #Number of layers\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i==num_layers-1:\n",
    "                h = BiRNN.get_layer(self.cell_type,self.state_sizes[i],return_sequences= False, activation = 'tanh')(h) #calling get_layer method of bi-directional network to run memory cell \n",
    "                                                                                                                         #when the current layer is the last one for the output shape\n",
    "            else:\n",
    "                h = BiRNN.get_layer(self.cell_type,self.state_sizes[i],return_sequences= True, activation = 'tanh')(h) #calling get_layer method of bi-directional network to run memory cell \n",
    "                                                                                                                         #when the current layer is not the last one for the output shape\n",
    "        \n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Run BiRNN for basic RNN ('basic_rnn') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 37s 705ms/step - loss: 0.3349 - accuracy: 0.8851 - val_loss: 0.1140 - val_accuracy: 0.9638\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 32s 606ms/step - loss: 0.0557 - accuracy: 0.9798 - val_loss: 0.1223 - val_accuracy: 0.9615\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 32s 619ms/step - loss: 0.0226 - accuracy: 0.9924 - val_loss: 0.1445 - val_accuracy: 0.9601\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 32s 606ms/step - loss: 0.0069 - accuracy: 0.9969 - val_loss: 0.1842 - val_accuracy: 0.9633\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 29s 560ms/step - loss: 6.1223e-04 - accuracy: 1.0000 - val_loss: 0.1275 - val_accuracy: 0.9720\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 30s 574ms/step - loss: 3.0196e-05 - accuracy: 1.0000 - val_loss: 0.1228 - val_accuracy: 0.9757\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 36s 685ms/step - loss: 7.2448e-04 - accuracy: 1.0000 - val_loss: 0.1286 - val_accuracy: 0.9665\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 33s 629ms/step - loss: 2.2844e-04 - accuracy: 1.0000 - val_loss: 0.1323 - val_accuracy: 0.9706\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 38s 733ms/step - loss: 3.0029e-06 - accuracy: 1.0000 - val_loss: 0.1312 - val_accuracy: 0.9734\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 36s 691ms/step - loss: 7.3902e-07 - accuracy: 1.0000 - val_loss: 0.1364 - val_accuracy: 0.9748\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 33s 633ms/step - loss: 1.2835e-07 - accuracy: 1.0000 - val_loss: 0.1446 - val_accuracy: 0.9761\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 34s 651ms/step - loss: 3.0063e-08 - accuracy: 1.0000 - val_loss: 0.1496 - val_accuracy: 0.9766\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 31s 593ms/step - loss: 1.3831e-08 - accuracy: 1.0000 - val_loss: 0.1516 - val_accuracy: 0.9766\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 29s 563ms/step - loss: 8.6354e-09 - accuracy: 1.0000 - val_loss: 0.1526 - val_accuracy: 0.9771\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 31s 598ms/step - loss: 5.8406e-09 - accuracy: 1.0000 - val_loss: 0.1524 - val_accuracy: 0.9775\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 29s 562ms/step - loss: 4.5148e-09 - accuracy: 1.0000 - val_loss: 0.1528 - val_accuracy: 0.9775\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 29s 562ms/step - loss: 3.1532e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9771\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 31s 587ms/step - loss: 2.5082e-09 - accuracy: 1.0000 - val_loss: 0.1531 - val_accuracy: 0.9771\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 32s 609ms/step - loss: 2.1499e-09 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9771\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 30s 581ms/step - loss: 1.4691e-09 - accuracy: 1.0000 - val_loss: 0.1530 - val_accuracy: 0.9771\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2235bd62c88>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN(None, embed_size=128, state_sizes=[128,128], data_manager=dm)#Simple RNN in bi-direction\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Run BiRNN for GRU ('gru') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 107s 2s/step - loss: 0.5526 - accuracy: 0.8096 - val_loss: 0.1578 - val_accuracy: 0.9440\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 100s 2s/step - loss: 0.0654 - accuracy: 0.9792 - val_loss: 0.1188 - val_accuracy: 0.9651\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 110s 2s/step - loss: 0.0209 - accuracy: 0.9927 - val_loss: 0.1592 - val_accuracy: 0.9674\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 103s 2s/step - loss: 0.0116 - accuracy: 0.9976 - val_loss: 0.0901 - val_accuracy: 0.9743\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 121s 2s/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.1129 - val_accuracy: 0.9734\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 111s 2s/step - loss: 1.6552e-04 - accuracy: 1.0000 - val_loss: 0.1610 - val_accuracy: 0.9706\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 109s 2s/step - loss: 4.7843e-05 - accuracy: 1.0000 - val_loss: 0.2028 - val_accuracy: 0.9734\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 110s 2s/step - loss: 1.6959e-06 - accuracy: 1.0000 - val_loss: 0.2088 - val_accuracy: 0.9729\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 109s 2s/step - loss: 3.1247e-07 - accuracy: 1.0000 - val_loss: 0.2208 - val_accuracy: 0.9739\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 113s 2s/step - loss: 7.5170e-08 - accuracy: 1.0000 - val_loss: 0.2262 - val_accuracy: 0.9748\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 111s 2s/step - loss: 3.2033e-08 - accuracy: 1.0000 - val_loss: 0.2272 - val_accuracy: 0.9752\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 109s 2s/step - loss: 1.9242e-08 - accuracy: 1.0000 - val_loss: 0.2287 - val_accuracy: 0.9757\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 109s 2s/step - loss: 1.4189e-08 - accuracy: 1.0000 - val_loss: 0.2297 - val_accuracy: 0.9752\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 118s 2s/step - loss: 1.1323e-08 - accuracy: 1.0000 - val_loss: 0.2309 - val_accuracy: 0.9752\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 114s 2s/step - loss: 8.8146e-09 - accuracy: 1.0000 - val_loss: 0.2321 - val_accuracy: 0.9748\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 108s 2s/step - loss: 7.4530e-09 - accuracy: 1.0000 - val_loss: 0.2333 - val_accuracy: 0.9748\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 107s 2s/step - loss: 6.0914e-09 - accuracy: 1.0000 - val_loss: 0.2343 - val_accuracy: 0.9748\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 109s 2s/step - loss: 5.2314e-09 - accuracy: 1.0000 - val_loss: 0.2351 - val_accuracy: 0.9752\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 120s 2s/step - loss: 4.6939e-09 - accuracy: 1.0000 - val_loss: 0.2359 - val_accuracy: 0.9752\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 128s 2s/step - loss: 4.1923e-09 - accuracy: 1.0000 - val_loss: 0.2366 - val_accuracy: 0.9752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2236c895668>"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN('gru', embed_size=128, state_sizes=[128,128], data_manager=dm)#GRU in bi-direction\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "**Run BiRNN for LSTM ('lstm') cell with $embed\\_size= 128, state\\_sizes= [128, 128], data\\_manager= dm$.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 52 steps, validate for 35 steps\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - 181s 3s/step - loss: 0.6723 - accuracy: 0.7680 - val_loss: 0.6450 - val_accuracy: 0.8147\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - 153s 3s/step - loss: 0.1257 - accuracy: 0.9655 - val_loss: 0.1077 - val_accuracy: 0.9665\n",
      "Epoch 3/20\n",
      "52/52 [==============================] - 157s 3s/step - loss: 0.0465 - accuracy: 0.9890 - val_loss: 0.1369 - val_accuracy: 0.9683\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - 161s 3s/step - loss: 0.0297 - accuracy: 0.9936 - val_loss: 0.1295 - val_accuracy: 0.9706\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - 169s 3s/step - loss: 0.0183 - accuracy: 0.9963 - val_loss: 0.1508 - val_accuracy: 0.9661\n",
      "Epoch 6/20\n",
      "52/52 [==============================] - 162s 3s/step - loss: 0.0041 - accuracy: 0.9994 - val_loss: 0.1421 - val_accuracy: 0.9720\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - 177s 3s/step - loss: 0.0014 - accuracy: 0.9997 - val_loss: 0.2012 - val_accuracy: 0.9651\n",
      "Epoch 8/20\n",
      "52/52 [==============================] - 175s 3s/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.2288 - val_accuracy: 0.9656\n",
      "Epoch 9/20\n",
      "52/52 [==============================] - 170s 3s/step - loss: 4.2031e-05 - accuracy: 1.0000 - val_loss: 0.2229 - val_accuracy: 0.9688\n",
      "Epoch 10/20\n",
      "52/52 [==============================] - 165s 3s/step - loss: 5.9090e-06 - accuracy: 1.0000 - val_loss: 0.2508 - val_accuracy: 0.9706\n",
      "Epoch 11/20\n",
      "52/52 [==============================] - 148s 3s/step - loss: 1.0117e-06 - accuracy: 1.0000 - val_loss: 0.2699 - val_accuracy: 0.9716\n",
      "Epoch 12/20\n",
      "52/52 [==============================] - 152s 3s/step - loss: 2.1471e-07 - accuracy: 1.0000 - val_loss: 0.2791 - val_accuracy: 0.9716\n",
      "Epoch 13/20\n",
      "52/52 [==============================] - 151s 3s/step - loss: 7.8861e-08 - accuracy: 1.0000 - val_loss: 0.2813 - val_accuracy: 0.9725\n",
      "Epoch 14/20\n",
      "52/52 [==============================] - 157s 3s/step - loss: 4.3925e-08 - accuracy: 1.0000 - val_loss: 0.2845 - val_accuracy: 0.9725\n",
      "Epoch 15/20\n",
      "52/52 [==============================] - 159s 3s/step - loss: 3.0058e-08 - accuracy: 1.0000 - val_loss: 0.2872 - val_accuracy: 0.9725\n",
      "Epoch 16/20\n",
      "52/52 [==============================] - 156s 3s/step - loss: 2.2538e-08 - accuracy: 1.0000 - val_loss: 0.2885 - val_accuracy: 0.9729\n",
      "Epoch 17/20\n",
      "52/52 [==============================] - 179s 3s/step - loss: 1.7343e-08 - accuracy: 1.0000 - val_loss: 0.2895 - val_accuracy: 0.9729\n",
      "Epoch 18/20\n",
      "52/52 [==============================] - 206s 4s/step - loss: 1.4189e-08 - accuracy: 1.0000 - val_loss: 0.2906 - val_accuracy: 0.9725\n",
      "Epoch 19/20\n",
      "52/52 [==============================] - 213s 4s/step - loss: 1.2183e-08 - accuracy: 1.0000 - val_loss: 0.2922 - val_accuracy: 0.9725\n",
      "Epoch 20/20\n",
      "52/52 [==============================] - 207s 4s/step - loss: 1.0714e-08 - accuracy: 1.0000 - val_loss: 0.2938 - val_accuracy: 0.9725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2238e0ea2b0>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_rnn = BiRNN('lstm', embed_size=128, state_sizes=[128,128], data_manager=dm)#LSTM in bi-direction\n",
    "bi_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "bi_rnn.compile_model(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "bi_rnn.fit(dm.tf_train_set.batch(64), epochs=20, validation_data = dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**Give your own comments about the performance of three memory cells for the dataset of interest as well as comparing BiRNN to UniRNN in Part 1.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiRNN() is the bidirectional network built with three memory cells such as basic, GRU and LSTM. The three memory cells of BiRNN perform very similar to each other just as for uni direction RNN network. Basic RNN memory cell reports the highest validation accuracy of 97.71% for BiRNN network while GRU and LSTM records 97.52% and 97.25% correspondingly. \n",
    "\n",
    "Overall, UniRNN and BiRNN have memory cells performing similar to each other (in the range between 97 and 98%). However, GRU cell of UniRNN network performs the best of all combination between network and memory cells. LSTM seemingly performs slight better in UniRNN network than in BiRNN. Additionally, Basic RNN cell gives better result for BiRNN as compared to what it gives for UniRNN.  \n",
    "\n",
    "\n",
    "Here, it seems the bi-directional approach in this case have not made too much difference in the performance of the various memory cells. RNN preserves information from embedding layer that has already passed through it using the hidden state. Unidirectional RNN only preserves information of the past because the only word embedding it has seen are from the past.\n",
    "\n",
    "Using bidirectional will run the embedding layer inputs in two ways, one from past to future and one from future to past and here, in the RNN that runs backwards you preserve information from the future and using the two hidden states combined you are able in any point in time to preserve information from both past and future. But in this case, the words given are short sequence which might be the reason bi-directional approach is not being too effective but it can be useful for longer sequences. However, the improvement using b-directional RNN can be seen for simple RNN memory cell which is giving better performances as compared to uni-directional. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">4.3. RNNs with various types, cells, and fine-tuning embedding matrix for sequence modeling and neural embedding </span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**In what follows, you are required to combine the code in Part 1 and Part 2 to gain a general RNN which can be either Uni-directional RNN or Bi-directional RNN and the embedding matrix can be initialized using a pretrained Word2Vect.**\n",
    "\n",
    "**Below are the descriptions of the attributes of the class *RNN*:**\n",
    "- `run_mode (self.run_mode)` has three values (scratch, init-only, and init-fine-tune).\n",
    "  - `scratch` means training the embedding matrix from scratch.\n",
    "  - `init-only` means only initialzing the embedding matrix with a pretrained Word2Vect but not further doing fine-tuning that matrix.\n",
    "  - `init-fine-tune` means both initialzing the embedding matrix with a pretrained Word2Vect and further doing fine-tuning that matrix.\n",
    "- `network_type (self.network_type)` has two values (uni-directional and bi-directional) which correspond to either Uni-directional RNN or Bi-directional RNN.\n",
    "- `cell_type (self.cell_type)` has three values (simple-rnn, gru, and lstm) which specify the memory cell used in the network.\n",
    "- `embed_model (self.embed_model)` specifes the pretrained Word2Vect model used.\n",
    "-  `embed_size (self.embed_size)` specifes the embedding size. Note that when run_mode is either init-only' or 'init-fine-tune', this embedding size is extracted from embed_model for dimension compatability.\n",
    "- `state_sizes (self.state_sizes)` indicates the list of the hidden sizes from the second hidden layers of memory cells. For example, $embed\\_size =128$ and $state\\_sizes = [64, 64]$ means that you have three hidden layers in your network with hidden sizes of $128, 64$ and $64$ respectively.\n",
    "\n",
    "**Complete the code of the class *RNN*.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, run_mode = 'scratch', cell_type= 'gru', network_type = 'uni-directional', embed_model= 'glove-wiki-gigaword-100', \n",
    "                 embed_size= 128, state_sizes = [64, 64], data_manager = None):\n",
    "        self.run_mode = run_mode\n",
    "        self.data_manager = data_manager\n",
    "        self.cell_type = cell_type\n",
    "        self.network_type = network_type\n",
    "        self.state_sizes = state_sizes\n",
    "        self.embed_model = embed_model\n",
    "        self.embed_size = embed_size\n",
    "        if self.run_mode != 'scratch':\n",
    "            self.embed_size = int(self.embed_model.split(\"-\")[-1])\n",
    "        self.data_manager = data_manager\n",
    "        self.vocab_size = dm.vocab_size +1\n",
    "        self.word2idx = dm.word2idx\n",
    "        self.word2vect = None\n",
    "        self.embed_matrix = np.zeros(shape= [self.vocab_size, self.embed_size])\n",
    "    \n",
    "    def build_embedding_matrix(self):\n",
    "        if os.path.exists(\"E.npy\"):  #if file exists\n",
    "            self.embed_matrix = np.load(\"E.npy\")           #Load the file for embedding matrix if existed\n",
    "        else: #file not existed or first-time run\n",
    "            self.word2vect = api.load(self.embed_model)   #load embedding model\n",
    "            for word, idx in self.word2idx.items():\n",
    "                try:\n",
    "                    self.embed_matrix[idx] = self.word2vect.word_vec(word)    #assign weight for the corresponding word and index\n",
    "                except KeyError: #word cannot be found\n",
    "                    pass\n",
    "            np.save(\"E.npy\", self.embed_matrix)\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_layer(cell_type= 'gru', network_type= 'uni-directional', hidden_size= 128, return_sequences= False, activation = 'tanh'):        \n",
    "        \n",
    "        if network_type == 'bi-directional': #If selected network is bi-directional, call the method get_layer from bi_rnn to pass the given arguments\n",
    "            return bi_rnn.get_layer(cell_type= cell_type, state_size= hidden_size, return_sequences= return_sequences, activation = activation)\n",
    "        \n",
    "        else: #call the method get_layer from Uni rnn to pass the given arguments\n",
    "            return UniRNN.get_layer(cell_type= cell_type, state_size= hidden_size, return_sequences= return_sequences, activation = activation)\n",
    "        \n",
    "        \n",
    "    def build(self):\n",
    "        x = tf.keras.layers.Input(shape=[None])\n",
    "        if self.run_mode == \"scratch\": #Build the embedding layer from scratch\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero= True, trainable= True)\n",
    "        \n",
    "        elif self.run_mode == \"init-only\": #Initialise embedding matrix but without fine-tuning\n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero= True, weights= [self.embed_matrix], trainable= False)\n",
    "                    \n",
    "        else: #fine-tuned after embeding_matrix\n",
    "            self.build_embedding_matrix()\n",
    "            self.embedding_layer = tf.keras.layers.Embedding(self.vocab_size, self.embed_size, mask_zero= True, weights= [self.embed_matrix], trainable= True)\n",
    "            \n",
    "            \n",
    "        h = self.embedding_layer(x)\n",
    "                \n",
    "        num_layers = len(self.state_sizes) #number of layers\n",
    "        \n",
    "        for i in range(num_layers):\n",
    "            if i==num_layers-1:\n",
    "                h = RNN.get_layer(self.cell_type, self.network_type, self.state_sizes[i],return_sequences= False, activation = 'tanh')(h) #calling get_layer method with arguments to run memory cell \n",
    "                                                                                                                                          #when the current layer is the last one for the output shape\n",
    "\n",
    "            else:\n",
    "                h = RNN.get_layer(self.cell_type, self.network_type, self.state_sizes[i],return_sequences= True, activation = 'tanh')(h) #calling get_layer method with arguments to run memory cell \n",
    "                                                                                                                                         #when the current layer is not the last one for the output shape\n",
    "                 \n",
    "        h = tf.keras.layers.Dense(dm.num_classes, activation='softmax')(h)\n",
    "        self.model = tf.keras.Model(inputs=x, outputs=h)\n",
    "        \n",
    "    \n",
    "    def compile_model(self, *args, **kwargs):\n",
    "        self.model.compile(*args, **kwargs)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        return self.model.fit(*args, **kwargs)\n",
    "    \n",
    "    def evaluate(self, *args, **kwargs):\n",
    "        self.model.evaluate(*args, **kwargs)       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**Design the experiment to compare three running modes. Note that you should stick with fixed values for other attributes and only vary *run_mode*. Give your comments for the results.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(6789)\n",
    "np.random.seed(6789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn1 = RNN(data_manager=dm, run_mode= \"scratch\") #Running from scratch\n",
    "rnn1.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn1.compile_model(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 77 steps, validate for 9 steps\n",
      "Epoch 1/20\n",
      "77/77 [==============================] - 40s 516ms/step - loss: 0.8300 - accuracy: 0.6982 - val_loss: 0.1962 - val_accuracy: 0.9450\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 30s 385ms/step - loss: 0.1019 - accuracy: 0.9709 - val_loss: 0.0752 - val_accuracy: 0.9761\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 29s 378ms/step - loss: 0.0372 - accuracy: 0.9904 - val_loss: 0.0597 - val_accuracy: 0.9872\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 31s 404ms/step - loss: 0.0182 - accuracy: 0.9955 - val_loss: 0.0539 - val_accuracy: 0.9817\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 28s 365ms/step - loss: 0.0078 - accuracy: 0.9982 - val_loss: 0.0495 - val_accuracy: 0.9853\n",
      "Epoch 6/20\n",
      "77/77 [==============================] - 28s 369ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0831 - val_accuracy: 0.9798\n",
      "Epoch 7/20\n",
      "77/77 [==============================] - 29s 372ms/step - loss: 0.0015 - accuracy: 0.9998 - val_loss: 0.1648 - val_accuracy: 0.9743\n",
      "Epoch 8/20\n",
      "77/77 [==============================] - 29s 371ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 0.0891 - val_accuracy: 0.9835\n",
      "Epoch 9/20\n",
      "77/77 [==============================] - 28s 363ms/step - loss: 0.0015 - accuracy: 0.9996 - val_loss: 0.0904 - val_accuracy: 0.9780\n",
      "Epoch 10/20\n",
      "77/77 [==============================] - 28s 362ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0922 - val_accuracy: 0.9817\n",
      "Epoch 11/20\n",
      "77/77 [==============================] - 28s 365ms/step - loss: 1.0247e-04 - accuracy: 1.0000 - val_loss: 0.0993 - val_accuracy: 0.9835\n",
      "Epoch 12/20\n",
      "77/77 [==============================] - 29s 372ms/step - loss: 4.2577e-05 - accuracy: 1.0000 - val_loss: 0.1321 - val_accuracy: 0.9798\n",
      "Epoch 13/20\n",
      "77/77 [==============================] - 28s 361ms/step - loss: 4.9941e-05 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9835\n",
      "Epoch 14/20\n",
      "77/77 [==============================] - 28s 364ms/step - loss: 5.3090e-06 - accuracy: 1.0000 - val_loss: 0.1461 - val_accuracy: 0.9798\n",
      "Epoch 15/20\n",
      "77/77 [==============================] - 29s 380ms/step - loss: 2.1056e-06 - accuracy: 1.0000 - val_loss: 0.1355 - val_accuracy: 0.9835\n",
      "Epoch 16/20\n",
      "77/77 [==============================] - 33s 427ms/step - loss: 4.2617e-07 - accuracy: 1.0000 - val_loss: 0.1313 - val_accuracy: 0.9853\n",
      "Epoch 17/20\n",
      "77/77 [==============================] - 29s 373ms/step - loss: 1.1051e-07 - accuracy: 1.0000 - val_loss: 0.1336 - val_accuracy: 0.9853\n",
      "Epoch 18/20\n",
      "77/77 [==============================] - 32s 413ms/step - loss: 6.7925e-08 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9872\n",
      "Epoch 19/20\n",
      "77/77 [==============================] - 28s 368ms/step - loss: 3.3781e-08 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 0.9890\n",
      "Epoch 20/20\n",
      "77/77 [==============================] - 29s 371ms/step - loss: 2.4214e-08 - accuracy: 1.0000 - val_loss: 0.1279 - val_accuracy: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x223e54ef710>"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn1.fit(dm.tf_train_set.batch(64), epochs=20, validation_data= dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 3s 318ms/step - loss: 0.1279 - accuracy: 0.9890\n"
     ]
    }
   ],
   "source": [
    "rnn1.evaluate(dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2 = RNN(data_manager=dm, run_mode= \"init-only\") #Running with init embedding matrix only\n",
    "rnn2.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn2.compile_model(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 77 steps, validate for 9 steps\n",
      "Epoch 1/20\n",
      "77/77 [==============================] - 81s 1s/step - loss: 0.9273 - accuracy: 0.6452 - val_loss: 0.3226 - val_accuracy: 0.9101\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 28s 364ms/step - loss: 0.2105 - accuracy: 0.9360 - val_loss: 0.1304 - val_accuracy: 0.9633\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 31s 402ms/step - loss: 0.1070 - accuracy: 0.9639 - val_loss: 0.1069 - val_accuracy: 0.9670\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 30s 388ms/step - loss: 0.0812 - accuracy: 0.9721 - val_loss: 0.0869 - val_accuracy: 0.9761\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 32s 420ms/step - loss: 0.0676 - accuracy: 0.9762 - val_loss: 0.0696 - val_accuracy: 0.9817\n",
      "Epoch 6/20\n",
      "77/77 [==============================] - 32s 413ms/step - loss: 0.0568 - accuracy: 0.9800 - val_loss: 0.0570 - val_accuracy: 0.9817\n",
      "Epoch 7/20\n",
      "77/77 [==============================] - 26s 336ms/step - loss: 0.0472 - accuracy: 0.9849 - val_loss: 0.0415 - val_accuracy: 0.9872\n",
      "Epoch 8/20\n",
      "77/77 [==============================] - 26s 338ms/step - loss: 0.0394 - accuracy: 0.9868 - val_loss: 0.0327 - val_accuracy: 0.9872\n",
      "Epoch 9/20\n",
      "77/77 [==============================] - 26s 336ms/step - loss: 0.0355 - accuracy: 0.9884 - val_loss: 0.0318 - val_accuracy: 0.9872\n",
      "Epoch 10/20\n",
      "77/77 [==============================] - 27s 356ms/step - loss: 0.0276 - accuracy: 0.9912 - val_loss: 0.0383 - val_accuracy: 0.9853\n",
      "Epoch 11/20\n",
      "77/77 [==============================] - 25s 327ms/step - loss: 0.0235 - accuracy: 0.9925 - val_loss: 0.0249 - val_accuracy: 0.9890\n",
      "Epoch 12/20\n",
      "77/77 [==============================] - 26s 333ms/step - loss: 0.0185 - accuracy: 0.9947 - val_loss: 0.0273 - val_accuracy: 0.9890\n",
      "Epoch 13/20\n",
      "77/77 [==============================] - 25s 328ms/step - loss: 0.0141 - accuracy: 0.9953 - val_loss: 0.0113 - val_accuracy: 0.9963\n",
      "Epoch 14/20\n",
      "77/77 [==============================] - 27s 348ms/step - loss: 0.0140 - accuracy: 0.9957 - val_loss: 0.0146 - val_accuracy: 0.9963\n",
      "Epoch 15/20\n",
      "77/77 [==============================] - 26s 342ms/step - loss: 0.0070 - accuracy: 0.9982 - val_loss: 0.0125 - val_accuracy: 0.9963\n",
      "Epoch 16/20\n",
      "77/77 [==============================] - 25s 331ms/step - loss: 0.0081 - accuracy: 0.9978 - val_loss: 0.0077 - val_accuracy: 0.9963\n",
      "Epoch 17/20\n",
      "77/77 [==============================] - 27s 346ms/step - loss: 0.0071 - accuracy: 0.9971 - val_loss: 0.0259 - val_accuracy: 0.9945\n",
      "Epoch 18/20\n",
      "77/77 [==============================] - 26s 343ms/step - loss: 0.0065 - accuracy: 0.9982 - val_loss: 0.0371 - val_accuracy: 0.9927\n",
      "Epoch 19/20\n",
      "77/77 [==============================] - 26s 341ms/step - loss: 0.0064 - accuracy: 0.9986 - val_loss: 0.0879 - val_accuracy: 0.9853\n",
      "Epoch 20/20\n",
      "77/77 [==============================] - 27s 345ms/step - loss: 0.0047 - accuracy: 0.9986 - val_loss: 0.0157 - val_accuracy: 0.9963\n",
      "9/9 [==============================] - 2s 252ms/step - loss: 0.0157 - accuracy: 0.9963\n"
     ]
    }
   ],
   "source": [
    "rnn2.model.fit(dm.tf_train_set.batch(64), epochs=20, validation_data= dm.tf_valid_set.batch(64))\n",
    "rnn2.evaluate(dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn3 = RNN(data_manager=dm, run_mode= \"init-fine-tune\") #Running with init embedding matrix\n",
    "                                                        #and fine-tuning\n",
    "rnn3.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "rnn3.compile_model(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 77 steps, validate for 9 steps\n",
      "Epoch 1/20\n",
      "77/77 [==============================] - 40s 518ms/step - loss: 0.8666 - accuracy: 0.6849 - val_loss: 0.2154 - val_accuracy: 0.9358\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 27s 355ms/step - loss: 0.1301 - accuracy: 0.9539 - val_loss: 0.0595 - val_accuracy: 0.9817\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 27s 346ms/step - loss: 0.0707 - accuracy: 0.9743 - val_loss: 0.0357 - val_accuracy: 0.9908\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 31s 406ms/step - loss: 0.0494 - accuracy: 0.9837 - val_loss: 0.0277 - val_accuracy: 0.9908\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 28s 365ms/step - loss: 0.0362 - accuracy: 0.9878 - val_loss: 0.0180 - val_accuracy: 0.9945\n",
      "Epoch 6/20\n",
      "77/77 [==============================] - 32s 420ms/step - loss: 0.0276 - accuracy: 0.9904 - val_loss: 0.0153 - val_accuracy: 0.9945\n",
      "Epoch 7/20\n",
      "77/77 [==============================] - 39s 505ms/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 0.0192 - val_accuracy: 0.9927\n",
      "Epoch 8/20\n",
      "77/77 [==============================] - 30s 393ms/step - loss: 0.0125 - accuracy: 0.9967 - val_loss: 0.0198 - val_accuracy: 0.9927\n",
      "Epoch 9/20\n",
      "77/77 [==============================] - 27s 354ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.0206 - val_accuracy: 0.9927\n",
      "Epoch 10/20\n",
      "77/77 [==============================] - 28s 361ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.0176 - val_accuracy: 0.9927\n",
      "Epoch 11/20\n",
      "77/77 [==============================] - 30s 390ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0182 - val_accuracy: 0.9945\n",
      "Epoch 12/20\n",
      "77/77 [==============================] - 29s 370ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0368 - val_accuracy: 0.9890\n",
      "Epoch 13/20\n",
      "77/77 [==============================] - 26s 344ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0086 - val_accuracy: 0.9982\n",
      "Epoch 14/20\n",
      "77/77 [==============================] - 25s 331ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0097 - val_accuracy: 0.9945\n",
      "Epoch 15/20\n",
      "77/77 [==============================] - 27s 350ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0501 - val_accuracy: 0.9908\n",
      "Epoch 16/20\n",
      "77/77 [==============================] - 29s 372ms/step - loss: 1.0825e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9927\n",
      "Epoch 17/20\n",
      "77/77 [==============================] - 29s 381ms/step - loss: 2.8571e-04 - accuracy: 0.9998 - val_loss: 0.0289 - val_accuracy: 0.9945\n",
      "Epoch 18/20\n",
      "77/77 [==============================] - 26s 341ms/step - loss: 8.0031e-06 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9927\n",
      "Epoch 19/20\n",
      "77/77 [==============================] - 28s 363ms/step - loss: 1.8161e-06 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 0.9982\n",
      "Epoch 20/20\n",
      "77/77 [==============================] - 27s 347ms/step - loss: 1.5866e-06 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 0.9963\n",
      "9/9 [==============================] - 2s 254ms/step - loss: 0.0327 - accuracy: 0.9963\n"
     ]
    }
   ],
   "source": [
    "rnn3.model.fit(dm.tf_train_set.batch(64), epochs=20, validation_data= dm.tf_valid_set.batch(64))\n",
    "rnn3.evaluate(dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above experiment is conducted to compare the performance for the three running models for the RNN model. For the better comparison, the best network types and memory cells achieved according to the performance of the corresponding model are used in this experiment. So, GRU memory cell, Uni directional RNN model and other arguments stay constant (such as embed size=128, state size = [64,64], data_manager=dm as above) for the experiment and it will compare for the run model of three different types which are:\n",
    "\n",
    "i) \"scratch\" means training the embedding matrix from scratch\n",
    "\n",
    "ii) \"init-only\" means only initialzing the embedding matrix with a pretrained Word2Vect but not further doing fine-tuning that matrix\n",
    "\n",
    "iii) \"init-fine-tune\" means both initialzing the embedding matrix with a pretrained Word2Vect and further doing fine-tuning that matrix\n",
    "\n",
    "Run mode type 'scratch' has the validation accuracy 98.9% which is pretty good performance considering the model training the embedding matrix from scratch. Initializing the embedding matrix here without fine-tuning provided the joint best results along with the embedding matrix and fine-tuning. Both of them are achieving the validation accuracy of 99.63% which is the best accuracy of any model in the experiment. However, fine-tuning the model allows us to learn the data in more depth which uses the trainable attribute. Therefore, it can be seen that initialising the embeding matrix and then fine-tuning the model provides more stability while training the model as compared to only initialising the matrix. Still, the final validation accuracy suggests that both results are qually best here.\n",
    "\n",
    "In the end, Run mode 'init-fine-tune' (which is initializing the embedding matrix and fine-tuning) provides the best result along with 'init-only' among all run mode type. Moreover, fine-tuning the embeding matrix does help in the RNN model learning various dimension of data which would provide best result. Here, the model with these two run type provides the accuracy rate of 99.63% which is impressive.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**Run the above general RNN with at least five parameter sets and try to obtain the best performance. You can stick with the running mode *init-fine-tune* and use grid search to tune other parameters. Record your best model which will be used in the next part.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The experiment is conducted to achieve the best model using the parameter given the best results in previous RNN construction. \n",
    "\n",
    "The results reported along with the model parameters are as following:\n",
    "\n",
    "Model 1:\n",
    "(data_manager=dm, run_mode= \"scratch\", cell_type= 'basic_rnn', network_type = 'bi-directional', embed_model= 'glove-wiki-gigaword-100', embed_size= 128, state_sizes = [64, 64])\n",
    ": accuracy = 98.87%\n",
    "\n",
    "Model 2: \n",
    "(data_manager=dm, run_mode= \"init-only\", cell_type= 'lstm', network_type = 'uni-directional', embed_model= 'glove-wiki-gigaword-100', embed_size= 128, state_sizes = [64, 64])\n",
    ": accuracy = 99.45%\n",
    "\n",
    "Model 3: \n",
    "(data_manager=dm, run_mode= \"init-only\", cell_type= 'basic_rnn', network_type = 'bi-directional', embed_model= 'glove-wiki-gigaword-100', embed_size= 128, state_sizes = [64, 64])\n",
    ": accuracy = 99.33%\n",
    "\n",
    "Model 4: \n",
    "(data_manager=dm, run_mode= \"init-fine-tune\", cell_type= 'gru', network_type = 'uni-directional', embed_model= 'glove-wiki-gigaword-100', embed_size= 128, state_sizes = [64, 64])\n",
    ": accuracy = 99.63%\n",
    "\n",
    "Model 5: \n",
    "(data_manager=dm, run_mode= \"init-fine-tune\", cell_type= 'lstm', network_type = 'bi-directional', embed_model= 'glove-wiki-gigaword-100', embed_size= 128, state_sizes = [128, 64])\n",
    ": accuracy = 99.40%\n",
    "\n",
    "Model 6: \n",
    "(data_manager=dm, run_mode=\"init-fine-tune\", cell_type= 'basic_rnn', network_type= 'bi-directional', embed_model='glove-wiki-gigaword-100', embed_size= 128, state_sizes = [128, 64])\n",
    ": accuracy = 98.29%\n",
    "\n",
    "Finally, we can conclude that the combinations of parameters used in model 4 provides the highest accuracy among all models, which is 99.63%. \n",
    "\n",
    "So, model 4 will be used as part of problem solving model in the next part.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The run of the best RNN model\n",
    "my_best_rnn = RNN(data_manager=dm, run_mode= \"init-fine-tune\", cell_type= 'gru', network_type = 'uni-directional', \n",
    "                  embed_model= 'glove-wiki-gigaword-100', embed_size= 128, state_sizes = [64, 64]) \n",
    "\n",
    "my_best_rnn.build()\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
    "my_best_rnn.compile_model(optimizer=opt, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 77 steps, validate for 9 steps\n",
      "Epoch 1/20\n",
      "77/77 [==============================] - 40s 518ms/step - loss: 0.8666 - accuracy: 0.6849 - val_loss: 0.2154 - val_accuracy: 0.9358\n",
      "Epoch 2/20\n",
      "77/77 [==============================] - 27s 355ms/step - loss: 0.1301 - accuracy: 0.9539 - val_loss: 0.0595 - val_accuracy: 0.9817\n",
      "Epoch 3/20\n",
      "77/77 [==============================] - 27s 346ms/step - loss: 0.0707 - accuracy: 0.9743 - val_loss: 0.0357 - val_accuracy: 0.9908\n",
      "Epoch 4/20\n",
      "77/77 [==============================] - 31s 406ms/step - loss: 0.0494 - accuracy: 0.9837 - val_loss: 0.0277 - val_accuracy: 0.9908\n",
      "Epoch 5/20\n",
      "77/77 [==============================] - 28s 365ms/step - loss: 0.0362 - accuracy: 0.9878 - val_loss: 0.0180 - val_accuracy: 0.9945\n",
      "Epoch 6/20\n",
      "77/77 [==============================] - 32s 420ms/step - loss: 0.0276 - accuracy: 0.9904 - val_loss: 0.0153 - val_accuracy: 0.9945\n",
      "Epoch 7/20\n",
      "77/77 [==============================] - 39s 505ms/step - loss: 0.0191 - accuracy: 0.9949 - val_loss: 0.0192 - val_accuracy: 0.9927\n",
      "Epoch 8/20\n",
      "77/77 [==============================] - 30s 393ms/step - loss: 0.0125 - accuracy: 0.9967 - val_loss: 0.0198 - val_accuracy: 0.9927\n",
      "Epoch 9/20\n",
      "77/77 [==============================] - 27s 354ms/step - loss: 0.0093 - accuracy: 0.9969 - val_loss: 0.0206 - val_accuracy: 0.9927\n",
      "Epoch 10/20\n",
      "77/77 [==============================] - 28s 361ms/step - loss: 0.0045 - accuracy: 0.9990 - val_loss: 0.0176 - val_accuracy: 0.9927\n",
      "Epoch 11/20\n",
      "77/77 [==============================] - 30s 390ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0182 - val_accuracy: 0.9945\n",
      "Epoch 12/20\n",
      "77/77 [==============================] - 29s 370ms/step - loss: 0.0029 - accuracy: 0.9990 - val_loss: 0.0368 - val_accuracy: 0.9890\n",
      "Epoch 13/20\n",
      "77/77 [==============================] - 26s 344ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0086 - val_accuracy: 0.9982\n",
      "Epoch 14/20\n",
      "77/77 [==============================] - 25s 331ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.0097 - val_accuracy: 0.9945\n",
      "Epoch 15/20\n",
      "77/77 [==============================] - 27s 350ms/step - loss: 0.0026 - accuracy: 0.9994 - val_loss: 0.0501 - val_accuracy: 0.9908\n",
      "Epoch 16/20\n",
      "77/77 [==============================] - 29s 372ms/step - loss: 1.0825e-04 - accuracy: 1.0000 - val_loss: 0.0233 - val_accuracy: 0.9927\n",
      "Epoch 17/20\n",
      "77/77 [==============================] - 29s 381ms/step - loss: 2.8571e-04 - accuracy: 0.9998 - val_loss: 0.0289 - val_accuracy: 0.9945\n",
      "Epoch 18/20\n",
      "77/77 [==============================] - 26s 341ms/step - loss: 8.0031e-06 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9927\n",
      "Epoch 19/20\n",
      "77/77 [==============================] - 28s 363ms/step - loss: 1.8161e-06 - accuracy: 1.0000 - val_loss: 0.0100 - val_accuracy: 0.9982\n",
      "Epoch 20/20\n",
      "77/77 [==============================] - 27s 347ms/step - loss: 1.5866e-06 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 0.9963\n",
      "9/9 [==============================] - 2s 254ms/step - loss: 0.0327 - accuracy: 0.9963\n"
     ]
    }
   ],
   "source": [
    "my_best_rnn.model.fit(dm.tf_train_set.batch(64), epochs=20, validation_data= dm.tf_valid_set.batch(64))\n",
    "my_best_rnn.evaluate(dm.tf_valid_set.batch(64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#0b486b\">4.4. Investigating the embedding vectors from the embedding matrix</span> ###\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red; font-weight:bold\"><span></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you know, the embedding matrix is a collection of embedding vectors, each is for one word. In this part, you will base on the cosine similarity of the embedding vectors for the words to find the top-k most relevant words for a given word.**\n",
    "\n",
    "**Good embeddings should have words close in meaning near each other by some similarity metrics. The similarity metric we'll use is the `consine` distance, which is defined for two vector $\\mathbf{u}$ and $\\mathbf{v}$ as $\\cos(\\mathbf{u}, \\mathbf{v})=\\frac{\\mathbf{u} \\cdot \\mathbf{v}}{\\left\\Vert{\\mathbf{u}}\\right\\Vert\\left\\Vert{\\mathbf{v}}\\right\\Vert}$ where $\\cdot$ means dot product and $\\left\\Vert\\cdot\\right\\Vert$ means the $\\mathcal{L}_2$ norm.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(u,v):\n",
    "    return np.dot(u,v)/(np.linalg.norm(u)*np.linalg.norm(u))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\"></span> \n",
    "\n",
    "**You are required to write the code for the function *find_most_similar(word= None, k=5, model= None)*. As its name, this function returns the top-k most relevant word for a given word based on the cosine similarity of the embedding vectors.**\n",
    "\n",
    "<div style=\"text-align: right\"><span style=\"color:red\"></span></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_similar(word= None, k=5, model= None):\n",
    "    \n",
    "    try:\n",
    "        #List for collecting all words, collecting the cosine similarity between words, Ranked words       \n",
    "        words_collect=[]\n",
    "        most_common =[]\n",
    "        top_common_words = []  \n",
    "        \n",
    "        for words, idx in model.word2idx.items():\n",
    "            words_collect+= [words] #Copying words into list\n",
    "        \n",
    "        for i in range(len(words_collect)):\n",
    "            if words_collect[i] == word: #Extractingt the index of the entered word to process\n",
    "                \n",
    "                for j in range(len(model.embed_matrix)):\n",
    "                    #Storing the cosine value between each words with given word\n",
    "                    most_common += [cosine_similarity(model.embed_matrix[i],model.embed_matrix[j])]\n",
    "        \n",
    "        top_n = sorted(range(len(most_common)), key=lambda n: most_common[n], reverse=True)\n",
    "        \n",
    "        top_index = top_n[1:k+1]#taking the index of first k words of the list for output\n",
    "        \n",
    "        for i in top_index:\n",
    "            top_common_words += [words_collect[i]] #First k words in the list\n",
    "        \n",
    "        if top_common_words == []:\n",
    "            raise Exception\n",
    "                    \n",
    "    except: #word not in the vocabulary\n",
    "        print(\"Word is not in the dictionary!\")\n",
    "        \n",
    "    return top_common_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the example of the above function. As you can observe, the result makes sense which demonstrates that we obtain a good model with the meaningful embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sinatra',\n",
       " 'mountains',\n",
       " 'spock',\n",
       " 'firm',\n",
       " 'ford',\n",
       " 'seaport',\n",
       " 'arcadia',\n",
       " 'exp',\n",
       " 'went',\n",
       " 'driven']"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_most_similar(word='poland',k=10,model=my_best_rnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
